
# ğŸ“ AI Career Guidance Chatbot using RAG

![Python](https://img.shields.io/badge/Python-3.10-blue.svg)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-orange)
![FastAPI](https://img.shields.io/badge/API-FastAPI-green)
![License](https://img.shields.io/badge/license-MIT-brightgreen)
![GitHub Actions](https://github.com/your-username/career-guidance-chatbot/actions/workflows/ci.yml/badge.svg)

An intelligent, real-time **RAG (Retrieval-Augmented Generation)** chatbot that helps students explore career options, learn required skills, and discover top courses â€” all powered by open-source LLMs and local embeddings.

---

## âœ¨ Features

âœ… Natural language Q&A about career paths  
âœ… Local knowledge base using ChromaDB  
âœ… LLM-powered explanations (Mistral or Hugging Face API)  
âœ… Streamlit chatbot UI  
âœ… FastAPI backend with REST endpoints  
âœ… Docker-ready, deployable on Hugging Face/Render  
âœ… Evaluation metrics (similarity, ROUGE-L)  
âœ… GitHub Actions CI for linting

---

## ğŸ§  How It Works

1. User asks a career-related question  
2. Query is embedded and matched with relevant documents using **ChromaDB**  
3. Retrieved context is passed to an **LLM** (local or remote)  
4. Final answer is generated and displayed with evaluation metrics  

---

## ğŸ›  Tech Stack

- `FastAPI` â€“ API backend  
- `Streamlit` â€“ Frontend chatbot  
- `sentence-transformers` â€“ Embedding model  
- `ChromaDB` â€“ Vector store for retrieval  
- `transformers` â€“ Local LLM (Mistral 7B Instruct)  
- `Hugging Face Inference API` â€“ Optional fallback  
- `Docker` â€“ For containerized deployment  

---

## ğŸš€ Getting Started

### 1. Clone the repo
```bash
git clone https://github.com/your-username/career-guidance-chatbot.git
cd career-guidance-chatbot
```

### 2. Install dependencies
```bash
pip install -r deployment/requirements.txt
```

### 3. Setup `.env`
```env
HUGGINGFACE_API_TOKEN=your_huggingface_token_here
```

### 4. Embed your dataset
```bash
python backend/embed_data.py
```

### 5. Run the app
```bash
uvicorn backend.main:app --reload
streamlit run frontend/app.py
```

---

## ğŸ³ Run with Docker

```bash
docker build -t career-chatbot .
docker run -p 8000:8000 career-chatbot
```

---

## ğŸ§ª Evaluation

The chatbot calculates:
- **Cosine Similarity** between query and retrieved content
- **ROUGE-L Score** to measure LLM output quality
- Logging of every query-response pair with timestamps

---

## ğŸ“ Project Structure

```
career-guidance-chatbot/
â”‚
â”œâ”€â”€ backend/        # FastAPI, RAG, logger, metrics
â”œâ”€â”€ frontend/       # Streamlit chatbot
â”œâ”€â”€ models/         # Embedding & LLM config
â”œâ”€â”€ data/           # Sample career data
â”œâ”€â”€ deployment/     # Dockerfile, requirements.txt
â”œâ”€â”€ notebooks/      # EDA and prototype work
â”œâ”€â”€ .github/        # CI workflows
â”œâ”€â”€ .env            # Secrets file
â””â”€â”€ main.py         # Entry point
```

---

## ğŸ“¸ Screenshots

> _(Add screenshots of chatbot UI, evaluation results, etc.)_

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributions

PRs are welcome! Please open an issue first to discuss proposed changes.

---

## ğŸ“¬ Contact

Built with â¤ï¸ by [Your Name](https://github.com/your-username)
